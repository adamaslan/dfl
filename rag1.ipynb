{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesnt save but optimized clean csv\n",
    "# import pandas as pd\n",
    "\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "def load_articles():\n",
    "    # Handle irregular CSV format\n",
    "    df = pd.read_csv('articles.csv', \n",
    "                    sep='\\t',  # Use tab separator if commas in content\n",
    "                    on_bad_lines='skip',\n",
    "                    header=None,\n",
    "                    names=['articles'])\n",
    "    \n",
    "    # Filter valid articles\n",
    "    return df[df['articles'].str.contains(r'\\w{50,}', na=False)]  # Only keep real content\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove navigation/footer patterns\n",
    "    patterns = [\n",
    "        r'Drinks Food.*?Subscribe',\n",
    "        r'Amaro Montenegro.*?MIA airport location',\n",
    "        r'Create Next App',\n",
    "        r'Follow us on Instagram\\s+HERE',\n",
    "        r'Check out their instagram.*?here'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "def process_articles():\n",
    "    df = load_articles()\n",
    "    df['clean_text'] = df['articles'].apply(clean_text)\n",
    "    return [text for text in df['clean_text'].unique() if len(text) > 500]\n",
    "\n",
    "# Process articles and save to CSV\n",
    "articles = process_articles()\n",
    "pd.DataFrame(articles, columns=['content']).to_csv('processed_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_98193/665401798.py:52: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018f46f8e5414ed09b655c892e17402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized FAISS index saved to: /Users/adamaslan/code/dfl/blog_faiss\n",
      "Processed 46 chunks from 15 articles\n",
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# works - creates vector store\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_optimized_vector_store(csv_path):\n",
    "    # 1. Load and preprocess articles with proper CSV handling\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        quoting=1,  # QUOTE_ALL\n",
    "        escapechar='\\\\',  # Handle escaped characters\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    # Ensure the 'articles' column exists\n",
    "    if 'articles' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain an 'articles' column\")\n",
    "    \n",
    "    articles = df['articles'].fillna('').tolist()\n",
    "    \n",
    "    # 2. Improved text splitting with context-aware chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Optimal for preserving paragraph context\n",
    "        chunk_overlap=100,  # Maintain context between chunks\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]  # Prioritize paragraph breaks\n",
    "    )\n",
    "    \n",
    "    # 3. Create document objects with metadata and error handling\n",
    "    documents = []\n",
    "    for idx, article in enumerate(articles):\n",
    "        try:\n",
    "            # Basic cleaning\n",
    "            cleaned_article = \" \".join(str(article).strip().split())\n",
    "            \n",
    "            chunks = text_splitter.split_text(cleaned_article)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                documents.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": f\"article_{idx}\", \"chunk\": i}\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process article {idx}: {str(e)}\")\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents were created from the input articles\")\n",
    "    \n",
    "    # 4. Initialize embeddings with batch processing\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'mps'},\n",
    "        encode_kwargs={\n",
    "            'normalize_embeddings': True,\n",
    "            'batch_size': 32  # Optimize for GPU/MPS performance\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 5. Create FAISS index with optimized parameters\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=\"COSINE\"  # Matches normalized embeddings\n",
    "    )\n",
    "    \n",
    "    # 6. Save with verification\n",
    "    save_path = os.path.abspath(\"blog_faiss\")\n",
    "    vector_store.save_local(save_path)\n",
    "    print(f\"Optimized FAISS index saved to: {save_path}\")\n",
    "    print(f\"Processed {len(documents)} chunks from {len(articles)} articles\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        vector_store = create_optimized_vector_store(\"articles2.csv\")\n",
    "        print(\"Vector store created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement MultiVectorRetriever (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for MultiVectorRetriever\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'filter_metadata' from 'langchain.vectorstores.utils' (/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/langchain/vectorstores/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#from langchain.retrievers import MultiVectorRetriever, EmbeddingRetriever\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filter_metadata\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'filter_metadata' from 'langchain.vectorstores.utils' (/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/langchain/vectorstores/utils.py)"
     ]
    }
   ],
   "source": [
    "# more metrics - better embeddings\n",
    "\n",
    "!pip install MultiVectorRetriever from langchain.retrievers.multi_vector\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import MultiVectorRetriever, EmbeddingRetriever\n",
    "from langchain.vectorstores.utils import filter_metadata\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def create_optimized_vector_store(csv_path):\n",
    "    # 1. Load and preprocess articles with proper CSV handling\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        quoting=1,  # QUOTE_ALL\n",
    "        escapechar='\\\\',\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    if 'articles' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain an 'articles' column\")\n",
    "    \n",
    "    articles = df['articles'].fillna('').tolist()\n",
    "    \n",
    "    # 2. Improved text splitting\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # 3. Create documents with metadata\n",
    "    documents = []\n",
    "    for idx, article in enumerate(articles):\n",
    "        try:\n",
    "            cleaned_article = \" \".join(str(article).strip().split())\n",
    "            chunks = text_splitter.split_text(cleaned_article)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                documents.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"source\": f\"article_{idx}\", \"chunk\": i}\n",
    "                ))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process article {idx}: {str(e)}\")\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents were created from the input articles\")\n",
    "    \n",
    "    # 4. Initialize embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'mps'},\n",
    "        encode_kwargs={\n",
    "            'normalize_embeddings': True,\n",
    "            'batch_size': 32\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 5. Create FAISS index\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=\"COSINE\"\n",
    "    )\n",
    "    \n",
    "    # 6. Save index\n",
    "    save_path = os.path.abspath(\"blog_faiss\")\n",
    "    vector_store.save_local(save_path)\n",
    "    print(f\"Optimized FAISS index saved to: {save_path}\")\n",
    "    print(f\"Processed {len(documents)} chunks from {len(articles)} articles\")\n",
    "    \n",
    "    # 7. Add LangChain enhancements\n",
    "    retriever = EmbeddingRetriever(vectorstore=vector_store)\n",
    "    multi_vector_retriever = MultiVectorRetriever(vectorstore=vector_store)\n",
    "    filtered_docs = filter_metadata(documents, {\"chunk\": 0})  # Example filter\n",
    "\n",
    "    # 8. Metrics block\n",
    "    print(\"\\nðŸ“Š Embedding & Retrieval Metrics:\")\n",
    "    try:\n",
    "        # Embedding density\n",
    "        sample_embeddings = embeddings.embed_documents([doc.page_content for doc in documents[:100]])\n",
    "        norms = [np.linalg.norm(vec) for vec in sample_embeddings]\n",
    "        avg_norm = np.mean(norms)\n",
    "\n",
    "        # Redundancy\n",
    "        unique_chunks = set(doc.page_content for doc in documents)\n",
    "        redundancy_ratio = 1 - len(unique_chunks) / len(documents)\n",
    "\n",
    "        # Retrieval latency\n",
    "        start = time.time()\n",
    "        _ = retriever.get_relevant_documents(\"sample query\")\n",
    "        latency = time.time() - start\n",
    "\n",
    "        print(f\"- Total chunks: {len(documents)}\")\n",
    "        print(f\"- Unique chunks: {len(unique_chunks)}\")\n",
    "        print(f\"- Redundancy ratio: {redundancy_ratio:.2f}\")\n",
    "        print(f\"- Avg embedding norm: {avg_norm:.4f}\")\n",
    "        print(f\"- Retrieval latency (sample query): {latency:.4f}s\")\n",
    "        print(f\"- Metadata-filtered docs (chunk=0): {len(filtered_docs)}\")\n",
    "        print(f\"- Coverage score: {len(documents) / len(articles):.2f} chunks/article\")\n",
    "        print(f\"- Semantic cohesion (adjacent chunk similarity): TBD\")\n",
    "        print(f\"- Recall@k / Precision@k: TBD (requires labeled queries)\")\n",
    "        print(f\"- Query drift: TBD (requires query-to-centroid comparison)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Metric computation error: {str(e)}\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        vector_store = create_optimized_vector_store(\"articles2.csv\")\n",
    "        print(\"Vector store created successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Generated Embeddings Shape: (100, 384)\n",
      "Optimized FAISS index saved as 'optimized_vector_database.faiss'.\n",
      "FAISS index contains 100 entries.\n",
      "\n",
      "Performing similarity search...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# with json - saves \n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the JSON file\n",
    "json_path = \"articles2.json\"\n",
    "with open(json_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filter out empty strings and extract valid content\n",
    "texts = [value for key, value in data.items() if value.strip()]\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "print(f\"Generated Embeddings Shape: {embeddings.shape}\")\n",
    "\n",
    "# Create a FAISS index with IDs\n",
    "vector_dim = embeddings.shape[1]  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(vector_dim)  # L2 distance (Euclidean)\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "\n",
    "# Assign unique IDs based on the JSON keys\n",
    "ids = np.array([int(key) for key in data.keys() if data[key].strip()], dtype=np.int64)\n",
    "index_with_ids.add_with_ids(embeddings, ids)\n",
    "\n",
    "# Save the FAISS index to a file\n",
    "faiss.write_index(index_with_ids, \"optimized_vector_database.faiss\")\n",
    "print(\"Optimized FAISS index saved as 'optimized_vector_database.faiss'.\")\n",
    "\n",
    "# Verify index size\n",
    "print(f\"FAISS index contains {index_with_ids.ntotal} entries.\")\n",
    "\n",
    "# Define the search function\n",
    "def search(query, top_k=5):\n",
    "    try:\n",
    "        # Encode the query\n",
    "        query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "        # Perform search in FAISS\n",
    "        distances, results = index_with_ids.search(query_embedding, top_k)\n",
    "        # Return results with distances\n",
    "        return [(texts[i], distances[0][j]) for j, i in enumerate(results[0])]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during search: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example search\n",
    "query = \"delicious Turkish dishes\"\n",
    "\n",
    "try:\n",
    "    print(\"\\nPerforming similarity search...\")\n",
    "    search_results = search(query, top_k=5)\n",
    "\n",
    "    print(\"\\nSimilarity Search Results:\")\n",
    "    for result, distance in search_results:\n",
    "        print(f\"Text: {result}\\nDistance: {distance}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while performing search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002'). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# claude - make faiss\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import gc  # For garbage collection\n",
    "\n",
    "def process_in_batches(json_path, batch_size=100):\n",
    "    # Initialize model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vector_dim = model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    # Initialize FAISS index\n",
    "    index = faiss.IndexFlatL2(vector_dim)\n",
    "    index_with_ids = faiss.IndexIDMap(index)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_texts = []\n",
    "    batch_ids = []\n",
    "    id_to_text = {}  # Store mapping of ID to text\n",
    "    current_id = 0\n",
    "    \n",
    "    print(\"Processing data in batches...\")\n",
    "    with open(json_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        for key, value in tqdm(data.items()):\n",
    "            if value.strip():  # Skip empty strings\n",
    "                batch_texts.append(value)\n",
    "                batch_ids.append(current_id)\n",
    "                id_to_text[current_id] = value\n",
    "                current_id += 1\n",
    "                \n",
    "                if len(batch_texts) >= batch_size:\n",
    "                    # Generate embeddings for batch\n",
    "                    embeddings = model.encode(batch_texts, convert_to_numpy=True)\n",
    "                    # Add to index\n",
    "                    index_with_ids.add_with_ids(embeddings, np.array(batch_ids, dtype=np.int64))\n",
    "                    \n",
    "                    # Clear batch and garbage collect\n",
    "                    batch_texts = []\n",
    "                    batch_ids = []\n",
    "                    gc.collect()\n",
    "    \n",
    "    # Process remaining items\n",
    "    if batch_texts:\n",
    "        embeddings = model.encode(batch_texts, convert_to_numpy=True)\n",
    "        index_with_ids.add_with_ids(embeddings, np.array(batch_ids, dtype=np.int64))\n",
    "    \n",
    "    # Save the index and mapping\n",
    "    faiss.write_index(index_with_ids, \"optimized_vector_database.faiss\")\n",
    "    with open(\"id_to_text_mapping.json\", \"w\") as f:\n",
    "        json.dump(id_to_text, f)\n",
    "    \n",
    "    return index_with_ids, id_to_text\n",
    "\n",
    "def search(query, index_with_ids, id_to_text, model, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, result_ids = index_with_ids.search(query_embedding, top_k)\n",
    "    \n",
    "    return [(id_to_text.get(str(id_), \"\"), distances[0][i]) \n",
    "            for i, id_ in enumerate(result_ids[0])]\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    json_path = \"articles2.json\"\n",
    "    index_with_ids, id_to_text = process_in_batches(json_path)\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    query = \"delicious Turkish dishes\"\n",
    "    results = search(query, index_with_ids, id_to_text, model)\n",
    "    \n",
    "    for text, distance in results:\n",
    "        print(f\"Distance: {distance}\\nText: {text}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# vector b\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"articles2.faiss\")\n",
    "\n",
    "# Generate some random query vectors (use real data if available)\n",
    "query_vectors = np.random.random((5, index.d)).astype(\"float32\")  # 5 queries\n",
    "\n",
    "# Perform a search on the index\n",
    "k = 10  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_vectors, k)\n",
    "\n",
    "# Display results\n",
    "print(\"Query Results:\")\n",
    "for i, (d, idx) in enumerate(zip(distances, indices)):\n",
    "    print(f\"Query {i+1}:\")\n",
    "    print(\"  Nearest neighbor distances:\", d)\n",
    "    print(\"  Nearest neighbor indices:\", idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves now but didnt show path before\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import pickle\n",
    "\n",
    "def create_vector_store(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'mps'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # # Save optimized index\n",
    "    # vector_store.save_local(\"blog_faiss\")\n",
    "    # return vector_store\n",
    "\n",
    "  # Save optimized index\n",
    "    save_path = os.path.abspath(\"blog_faiss\")  # Get absolute path\n",
    "    vector_store.save_local(save_path)\n",
    "    print(f\"FAISS vector store saved to: {save_path}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m articles \u001b[38;5;241m=\u001b[39m process_articles()  \u001b[38;5;66;03m# Use your existing `process_articles()` function\u001b[39;00m\n\u001b[1;32m     35\u001b[0m chunks \u001b[38;5;241m=\u001b[39m split_text_into_chunks(articles)\n\u001b[0;32m---> 36\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mcreate_vector_store\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m     18\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n\u001b[1;32m     19\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m},  \u001b[38;5;66;03m# Use Metal Performance Shaders (MPS) for macOS\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     encode_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create FAISS vector store\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save FAISS index to project root\u001b[39;00m\n\u001b[1;32m     28\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblog_faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1044\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1001\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m   1002\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[1;32m   1003\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "\n",
    "def split_text_into_chunks(articles, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = []\n",
    "    for article in articles:\n",
    "        chunks.extend(text_splitter.split_text(article))\n",
    "    return chunks\n",
    "\n",
    "def create_vector_store(chunks):\n",
    "    # Initialize embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'mps'},  # Use Metal Performance Shaders (MPS) for macOS\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    \n",
    "    # Save FAISS index to project root\n",
    "    vector_store.save_local(\"blog_faiss\")\n",
    "    print(\"FAISS vector store saved to 'blog_faiss'\")\n",
    "    return vector_store\n",
    "\n",
    "# Usage:\n",
    "# Assuming 'articles' is a list of cleaned article content\n",
    "articles = process_articles()  # Use your existing `process_articles()` function\n",
    "chunks = split_text_into_chunks(articles)\n",
    "vector_store = create_vector_store(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Initialize components\n",
    "from typing import List\n",
    "from ollama import Client\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'mps'}  # Use Apple Metal\n",
    ")\n",
    "\n",
    "# Cell 3: RAG Class\n",
    "class BlogRAG:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.client = Client(host='http://localhost:11434')\n",
    "            self.retriever = FAISS.load_local(\n",
    "                \"blog_faiss\",\n",
    "                embeddings\n",
    "            ).as_retriever(search_kwargs={\"k\": 3})\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize RAG system: {str(e)}\")\n",
    "\n",
    "    def format_prompt(self, context: List[str], question: str) -> str:\n",
    "        return f\"\"\"Answer using ONLY these blog excerpts:\n",
    "        {\"\".join([f'â€¢ {doc.page_content}\\n' for doc in context])}\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer in the blog's casual food writing style using under 200 words:\"\"\"\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        try:\n",
    "            context = self.retriever.get_relevant_documents(question)\n",
    "            prompt = self.format_prompt(context, question)\n",
    "            \n",
    "            response = self.client.generate(\n",
    "                model='phi3',\n",
    "                prompt=prompt,\n",
    "                options={'temperature': 0.3, 'num_ctx': 2048}\n",
    "            )\n",
    "            return response['response']\n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "# Cell 4: Test the system\n",
    "if __name__ == \"__main__\":\n",
    "    rag = BlogRAG()\n",
    "    sample_question = \"Where can I get good Turkish food in New York?\"\n",
    "    print(f\"Question: {sample_question}\")\n",
    "    print(\"Answer:\", rag.ask(sample_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17065) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (3.3.1)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (2.4.1.post3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (3.11.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (0.3.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (0.2.10)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee03db8a8564357ad7f423290d26e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722d99296eff42408610e6366dc176d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7baf166e6f4ecd96e1ca7bef55608d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159cce5deafe49369de3543f949e3c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19797d09b6c244a7b273ac895c9bd7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c8cc1bf2df4704982e5568c3b56711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/64/2b642798915fc368e7b638986f68446b121c1d59b30075e146bd6312ee664ac2/6e6001da2106d4757498752a021df6c2bdc332c650aae4bae6b0c004dcf14933?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738037249&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODAzNzI0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiLzY0LzJiNjQyNzk4OTE1ZmMzNjhlN2I2Mzg5ODZmNjg0NDZiMTIxYzFkNTliMzAwNzVlMTQ2YmQ2MzEyZWU2NjRhYzIvNmU2MDAxZGEyMTA2ZDQ3NTc0OTg3NTJhMDIxZGY2YzJiZGMzMzJjNjUwYWFlNGJhZTZiMGMwMDRkY2YxNDkzMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IljEF7u50InTXhQKUxf%7EQBC7I2DAtZwyf2wQtdMriXaSSJFJGjfssqwsw05xfQabR6hPJ7pfzuZ1jMUAPIGVfj7OR-I6Tz81rT2jd8jlWOtvwBH-yQ0szNXn7ukOylx3ryxJw3%7EKhTAJyCqcIu1A%7EtdQx6EkJK-l2UlNMm0kiv8GRsIgqUBU03zImj5Q5rICEZWLVoyijQu2S9PKiSd5A0Qz6%7ECtPdZEOEjdZgudXGbRweCI8DTFO91VftvyYGmHWYjH-AGYKO3vUoibCxnWwE3EDYiEuTLKVcpG2A-MJ3kcmN98qbl-1-hXQMQwIG3xr6x3aqrq0uXyr2faiX9D2A__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118b213633b94e699c7a25602dd6240a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   1%|          | 21.0M/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/64/2b642798915fc368e7b638986f68446b121c1d59b30075e146bd6312ee664ac2/6e6001da2106d4757498752a021df6c2bdc332c650aae4bae6b0c004dcf14933?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738037249&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODAzNzI0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiLzY0LzJiNjQyNzk4OTE1ZmMzNjhlN2I2Mzg5ODZmNjg0NDZiMTIxYzFkNTliMzAwNzVlMTQ2YmQ2MzEyZWU2NjRhYzIvNmU2MDAxZGEyMTA2ZDQ3NTc0OTg3NTJhMDIxZGY2YzJiZGMzMzJjNjUwYWFlNGJhZTZiMGMwMDRkY2YxNDkzMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IljEF7u50InTXhQKUxf%7EQBC7I2DAtZwyf2wQtdMriXaSSJFJGjfssqwsw05xfQabR6hPJ7pfzuZ1jMUAPIGVfj7OR-I6Tz81rT2jd8jlWOtvwBH-yQ0szNXn7ukOylx3ryxJw3%7EKhTAJyCqcIu1A%7EtdQx6EkJK-l2UlNMm0kiv8GRsIgqUBU03zImj5Q5rICEZWLVoyijQu2S9PKiSd5A0Qz6%7ECtPdZEOEjdZgudXGbRweCI8DTFO91VftvyYGmHWYjH-AGYKO3vUoibCxnWwE3EDYiEuTLKVcpG2A-MJ3kcmN98qbl-1-hXQMQwIG3xr6x3aqrq0uXyr2faiX9D2A__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8aebfe8944b63ac3b98bad4a8afb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  38%|###7      | 828M/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/64/2b642798915fc368e7b638986f68446b121c1d59b30075e146bd6312ee664ac2/6e6001da2106d4757498752a021df6c2bdc332c650aae4bae6b0c004dcf14933?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738037249&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODAzNzI0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiLzY0LzJiNjQyNzk4OTE1ZmMzNjhlN2I2Mzg5ODZmNjg0NDZiMTIxYzFkNTliMzAwNzVlMTQ2YmQ2MzEyZWU2NjRhYzIvNmU2MDAxZGEyMTA2ZDQ3NTc0OTg3NTJhMDIxZGY2YzJiZGMzMzJjNjUwYWFlNGJhZTZiMGMwMDRkY2YxNDkzMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IljEF7u50InTXhQKUxf%7EQBC7I2DAtZwyf2wQtdMriXaSSJFJGjfssqwsw05xfQabR6hPJ7pfzuZ1jMUAPIGVfj7OR-I6Tz81rT2jd8jlWOtvwBH-yQ0szNXn7ukOylx3ryxJw3%7EKhTAJyCqcIu1A%7EtdQx6EkJK-l2UlNMm0kiv8GRsIgqUBU03zImj5Q5rICEZWLVoyijQu2S9PKiSd5A0Qz6%7ECtPdZEOEjdZgudXGbRweCI8DTFO91VftvyYGmHWYjH-AGYKO3vUoibCxnWwE3EDYiEuTLKVcpG2A-MJ3kcmN98qbl-1-hXQMQwIG3xr6x3aqrq0uXyr2faiX9D2A__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f102a3fae5b0451ea516239c630db090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  45%|####4     | 986M/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/2b/64/2b642798915fc368e7b638986f68446b121c1d59b30075e146bd6312ee664ac2/6e6001da2106d4757498752a021df6c2bdc332c650aae4bae6b0c004dcf14933?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1738037249&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODAzNzI0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzJiLzY0LzJiNjQyNzk4OTE1ZmMzNjhlN2I2Mzg5ODZmNjg0NDZiMTIxYzFkNTliMzAwNzVlMTQ2YmQ2MzEyZWU2NjRhYzIvNmU2MDAxZGEyMTA2ZDQ3NTc0OTg3NTJhMDIxZGY2YzJiZGMzMzJjNjUwYWFlNGJhZTZiMGMwMDRkY2YxNDkzMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IljEF7u50InTXhQKUxf%7EQBC7I2DAtZwyf2wQtdMriXaSSJFJGjfssqwsw05xfQabR6hPJ7pfzuZ1jMUAPIGVfj7OR-I6Tz81rT2jd8jlWOtvwBH-yQ0szNXn7ukOylx3ryxJw3%7EKhTAJyCqcIu1A%7EtdQx6EkJK-l2UlNMm0kiv8GRsIgqUBU03zImj5Q5rICEZWLVoyijQu2S9PKiSd5A0Qz6%7ECtPdZEOEjdZgudXGbRweCI8DTFO91VftvyYGmHWYjH-AGYKO3vUoibCxnWwE3EDYiEuTLKVcpG2A-MJ3kcmN98qbl-1-hXQMQwIG3xr6x3aqrq0uXyr2faiX9D2A__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4534a1232054a039c8dc02c5e2cbdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  45%|####5     | 996M/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1485f7a59440bfb268bcf2d479ac06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_11819/1148055367.py:35: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Vector store load failed: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open blog_faiss/index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_11819/1148055367.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mVector store load failed: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.faiss\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11321\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11322\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open blog_faiss/index.faiss for reading: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Cell 4: Test\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     rag \u001b[38;5;241m=\u001b[39m \u001b[43mBlogRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     sample_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere can I find good Turkish food in NYC?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mBlogRAG.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector store load failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Vector store load failed: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open blog_faiss/index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install requirements\n",
    "%pip install langchain sentence-transformers faiss-cpu transformers torch\n",
    "\n",
    "# Cell 2: Initialize components\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from typing import List\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'mps' for Apple Silicon\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Initialize local LLM (using a smaller public model)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Cell 3: RAG Class\n",
    "class BlogRAG:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.vector_store = FAISS.load_local(\n",
    "                \"blog_faiss\",\n",
    "                embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Vector store load failed: {str(e)}\")\n",
    "\n",
    "    def format_prompt(self, context: List[str], question: str) -> str:\n",
    "        return f\"\"\"Based on these food blog excerpts:\n",
    "        {\" \".join([doc.page_content[:500] for doc in context])}\n",
    "        \n",
    "        Answer this question: {question}\n",
    "        Keep the answer under 150 words and casual:\"\"\"\n",
    "    \n",
    "    def ask(self, question: str) -> str:\n",
    "        try:\n",
    "            context = self.retriever.get_relevant_documents(question)\n",
    "            prompt = self.format_prompt(context, question)\n",
    "            \n",
    "            response = llm(\n",
    "                prompt,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True\n",
    "            )\n",
    "            return response[0]['generated_text'].split(\"casual:\")[-1].strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "# Cell 4: Test\n",
    "if __name__ == \"__main__\":\n",
    "    rag = BlogRAG()\n",
    "    sample_question = \"Where can I find good Turkish food in NYC?\"\n",
    "    print(f\"Question: {sample_question}\")\n",
    "    print(\"Answer:\", rag.ask(sample_question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
