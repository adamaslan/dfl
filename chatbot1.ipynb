{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5504a65518d34c1394e2196eeaa4a1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  14%|#3        | 283M/2.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0e7185dee248d488323d53d77f18d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RagTokenizer' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     50\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mSimpleDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m tokenizer(label, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids,\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: (input_ids \u001b[38;5;241m!=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m)\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: label_ids\n\u001b[1;32m     30\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RagTokenizer' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Simulate a dataset\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_texts, labels):\n",
    "        self.input_texts = input_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        label_ids = tokenizer(label, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": (input_ids != tokenizer.pad_token_id).long(),\n",
    "            \"labels\": label_ids\n",
    "        }\n",
    "\n",
    "# Example input and labels\n",
    "input_texts = [\"How to train a model?\", \"What is transformers library?\"]\n",
    "labels = [\"You can use various algorithms.\", \"It is a Python library.\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SimpleDataset(input_texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True)  # Ensure batch size is a multiple of 5\n",
    "\n",
    "# Set n_docs to match the batch size\n",
    "n_docs = 5\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Create doc_scores (simulated as zeros, since we're not using a retriever)\n",
    "        doc_scores = torch.zeros(input_ids.size(0), 1).to(device)  # 1 document per batch\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "            doc_scores=doc_scores,\n",
    "            n_docs=n_docs,  # Pass n_docs here\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m config\u001b[38;5;241m.\u001b[39muse_dummy_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Initialize retriever with configuration and trust_remote_code\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mRagRetriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_dummy_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added this parameter\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model \u001b[38;5;241m=\u001b[39m RagTokenForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/retrieval_rag.py:451\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[0;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     index \u001b[38;5;241m=\u001b[39m CustomHFIndex(config\u001b[38;5;241m.\u001b[39mretrieval_vector_size, indexed_dataset)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    453\u001b[0m     config,\n\u001b[1;32m    454\u001b[0m     question_encoder_tokenizer\u001b[38;5;241m=\u001b[39mquestion_encoder_tokenizer,\n\u001b[1;32m    455\u001b[0m     generator_tokenizer\u001b[38;5;241m=\u001b[39mgenerator_tokenizer,\n\u001b[1;32m    456\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m    457\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/retrieval_rag.py:424\u001b[0m, in \u001b[0;36mRagRetriever._build_index\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LegacyIndex(\n\u001b[1;32m    420\u001b[0m         config\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[1;32m    421\u001b[0m         config\u001b[38;5;241m.\u001b[39mindex_path \u001b[38;5;129;01mor\u001b[39;00m LEGACY_INDEX_PATH,\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mindex_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCustomHFIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieval_vector_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassages_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CanonicalHFIndex(\n\u001b[1;32m    431\u001b[0m         vector_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mretrieval_vector_size,\n\u001b[1;32m    432\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m         dataset_revision\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset_revision,\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/retrieval_rag.py:330\u001b[0m, in \u001b[0;36mCustomHFIndex.load_from_disk\u001b[0;34m(cls, vector_size, dataset_path, index_path)\u001b[0m\n\u001b[1;32m    328\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading passages from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m index_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `dataset.get_index(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).save(index_path)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_from_disk(dataset_path)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(vector_size\u001b[38;5;241m=\u001b[39mvector_size, dataset\u001b[38;5;241m=\u001b[39mdataset, index_path\u001b[38;5;241m=\u001b[39mindex_path)\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide `dataset_path` and `index_path` after calling `dataset.save_to_disk(dataset_path)` and `dataset.get_index('embeddings').save(index_path)`."
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer, RagRetriever, RagConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"facebook/rag-sequence-nq\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 1e-5\n",
    "}\n",
    "\n",
    "# Cell 2: Dataset Class and Loading\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        # Pre-tokenize the dataset\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing dataset\"):\n",
    "            try:\n",
    "                article = row[\"articles\"]\n",
    "                question = \"What is the content of this article?\"\n",
    "                \n",
    "                # Tokenize input\n",
    "                inputs = self.tokenizer.question_encoder(\n",
    "                    question,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # Tokenize target\n",
    "                targets = self.tokenizer(\n",
    "                    article,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                self.inputs.append({\n",
    "                    \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "                })\n",
    "                self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load model components\n",
    "tokenizer = RagTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Configure RAG\n",
    "config = RagConfig.from_pretrained(CONFIG[\"model_name\"])\n",
    "config.index_name = \"custom\"  # Changed from \"exact\"\n",
    "config.use_dummy_dataset = True\n",
    "\n",
    "# Initialize retriever with configuration and trust_remote_code\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    index_name=\"custom\",\n",
    "    use_dummy_dataset=True,\n",
    "    config=config,\n",
    "    trust_remote_code=True  # Added this parameter\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = RagTokenForGeneration.from_pretrained(CONFIG[\"model_name\"], config=config)\n",
    "model.set_retriever(retriever)\n",
    "\n",
    "# Load dataset\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "dataset = CQADataset(articles, tokenizer, max_length=CONFIG[\"max_length\"])\n",
    "dataloader = DataLoader(dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Cell 3: Training Loop\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save_pretrained(\"models/rag_model_final\")\n",
    "tokenizer.save_pretrained(\"models/rag_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    653\u001b[0m         )\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m         )\n\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         )\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    670\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n",
      "\u001b[1;32m     89\u001b[0m context_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(attention_mask)  \u001b[38;5;66;03m# Empty context mask\u001b[39;00m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Perform forward pass with empty context input and mask\u001b[39;00m\n",
      "\u001b[0;32m---> 92\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m    101\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:1322\u001b[0m, in \u001b[0;36mRagTokenForGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, context_input_ids, context_attention_mask, doc_scores, use_cache, output_attentions, output_hidden_states, output_retrieved, do_marginalize, reduce_loss, labels, n_docs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1319\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m labels\n",
      "\u001b[1;32m   1320\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoc_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_retrieved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_retrieved\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_docs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1339\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1340\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/rag/modeling_rag.py:658\u001b[0m, in \u001b[0;36mRagModel.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, doc_scores, context_input_ids, context_attention_mask, use_cache, output_attentions, output_hidden_states, output_retrieved, n_docs)\u001b[0m\n",
      "\u001b[1;32m    650\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_input_ids` are passed, if no `retriever` is set. Alternatively, you can\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    653\u001b[0m         )\n",
      "\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m context_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `context_attention_mask` are passed, if no `retriever` is set. Alternatively, you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    656\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m can set a retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    657\u001b[0m         )\n",
      "\u001b[0;32m--> 658\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n",
      "\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m retriever using the `set_retriever(...)` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    661\u001b[0m         )\n",
      "\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n",
      "\u001b[1;32m    664\u001b[0m     doc_scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    665\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (doc_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m n_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n",
      "\u001b[1;32m    668\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The first dimension of `context_input_ids` should be a multiple of `n_docs`=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    669\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    670\u001b[0m )\n",
      "\n",
      "\u001b[0;31mAssertionError\u001b[0m: Make sure that `doc_scores` are passed, if no `retriever` is set. Alternatively, you can set a retriever using the `set_retriever(...)` function."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RagTokenForGeneration, RagTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load RAG model and tokenizer\n",
    "model_name = \"facebook/rag-sequence-nq\"\n",
    "model = RagTokenForGeneration.from_pretrained(model_name)\n",
    "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Pre-tokenize the entire dataset to avoid repeated tokenization in each epoch\n",
    "        for _, row in df.iterrows():\n",
    "            article = row[\"articles\"]\n",
    "\n",
    "            # Create a dummy question for each article (you can customize this)\n",
    "            question = \"What is the content of this article?\"\n",
    "\n",
    "            # Dummy answer can be the article itself or a summary, depending on your use case\n",
    "            answer = article\n",
    "\n",
    "            input_text = f\"Context: {article} Reasoning: {question}\"\n",
    "            target_text = answer\n",
    "\n",
    "            # Tokenize input and target texts and store the results\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            self.inputs.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            })\n",
    "            self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Load dataset (articles only)\n",
    "articles = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CQADataset(articles, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # Define the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Create empty context tensors to disable the retrieval mechanism\n",
    "        context_input_ids = torch.zeros_like(input_ids)  # Empty context input\n",
    "        context_attention_mask = torch.zeros_like(attention_mask)  # Empty context mask\n",
    "\n",
    "        # Perform forward pass with empty context input and mask\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            context_input_ids=context_input_ids,\n",
    "            context_attention_mask=context_attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
