{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5088127f3f840a684e4c30412cc09b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9d307089d74c74994056662bf73ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca293ed0bca48309ee2d9552adf85bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0810946ef3c4e328b4cf0e80d0983fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Dataset Preparation\n",
    "import torch\n",
    "from transformers import (\n",
    "    RagTokenForGeneration, \n",
    "    RagTokenizer, \n",
    "    RagRetriever, \n",
    "    RagConfig, \n",
    "    DPRQuestionEncoder,\n",
    "    DPRQuestionEncoderTokenizer\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import os\n",
    "from datasets import Dataset as HFDataset\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model_name\": \"facebook/rag-sequence-nq\",\n",
    "    \"question_encoder_name\": \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    \"max_length\": 512,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"dataset_path\": \"custom_dataset\",\n",
    "    \"index_path\": \"custom_index.faiss\"\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"custom_dataset\", exist_ok=True)\n",
    "\n",
    "# Load and prepare passages\n",
    "articles_df = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "custom_dataset = HFDataset.from_pandas(\n",
    "    pd.DataFrame({\n",
    "        'text': articles_df['articles'].tolist(),\n",
    "        'title': [f\"Article {i}\" for i in range(len(articles_df))],\n",
    "        'id': list(range(len(articles_df)))\n",
    "    })\n",
    ")\n",
    "\n",
    "# Initialize question encoder and tokenizer\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(CONFIG[\"question_encoder_name\"])\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(CONFIG[\"question_encoder_name\"])\n",
    "\n",
    "# Function to compute embeddings\n",
    "def compute_embeddings(batch):\n",
    "    # First tokenize the texts\n",
    "    encodings = question_encoder_tokenizer(\n",
    "        batch['text'],\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Then get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = question_encoder(\n",
    "            input_ids=encodings['input_ids'],\n",
    "            attention_mask=encodings['attention_mask']\n",
    "        )\n",
    "        embeddings = outputs.pooler_output\n",
    "    \n",
    "    return {'embeddings': embeddings.numpy()}\n",
    "\n",
    "# Add embeddings to dataset\n",
    "custom_dataset = custom_dataset.map(\n",
    "    compute_embeddings,\n",
    "    batched=True,\n",
    "    batch_size=CONFIG[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = 768  # DPR embedding dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "embeddings_array = np.array(custom_dataset['embeddings'])\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Save dataset and index\n",
    "custom_dataset.save_to_disk(CONFIG[\"dataset_path\"])\n",
    "faiss.write_index(index, CONFIG[\"index_path\"])\n",
    "\n",
    "# Cell 2: Dataset Class and Model Setup\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing dataset\"):\n",
    "            try:\n",
    "                article = row[\"articles\"]\n",
    "                question = \"What is the content of this article?\"\n",
    "                \n",
    "                inputs = self.tokenizer.question_encoder(\n",
    "                    question,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    article,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                self.inputs.append({\n",
    "                    \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "                })\n",
    "                self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Initialize RAG components\n",
    "tokenizer = RagTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "config = RagConfig.from_pretrained(CONFIG[\"model_name\"])\n",
    "config.index_name = \"custom\"\n",
    "config.passages_path = CONFIG[\"dataset_path\"]\n",
    "config.index_path = CONFIG[\"index_path\"]\n",
    "\n",
    "# Initialize retriever with custom dataset and index\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    index_name=\"custom\",\n",
    "    passages_path=CONFIG[\"dataset_path\"],\n",
    "    index_path=CONFIG[\"index_path\"],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = RagTokenForGeneration.from_pretrained(CONFIG[\"model_name\"], config=config)\n",
    "model.set_retriever(retriever)\n",
    "\n",
    "# Prepare dataset for training\n",
    "train_dataset = CQADataset(articles_df, tokenizer, max_length=CONFIG[\"max_length\"])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Cell 3: Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save_pretrained(\"models/rag_model_final\")\n",
    "tokenizer.save_pretrained(\"models/rag_model_final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
