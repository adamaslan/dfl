{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 17:49:46,968 - INFO - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "2025-03-01 17:49:46,968 - INFO - \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-03-01 17:49:46,968 - INFO -  * Restarting with stat\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 692, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 311, in bind\n",
      "    super().bind(addr)\n",
      "  File \"_zmq.py\", line 917, in zmq.backend.cython._zmq.Socket.bind\n",
      "  File \"_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9002')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging  \n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "app = Flask(__name__)\n",
    "# CORS(app, resources={\n",
    "#     r\"/generate\": {\n",
    "#         \"origins\": [\"http://localhost:3000\"],\n",
    "#         \"methods\": [\"POST\", \"OPTIONS\"],\n",
    "#         \"allow_headers\": [\"Content-Type\"]\n",
    "#     }\n",
    "# })\n",
    "\n",
    "CORS(app, resources={r\"/generate\": {\"origins\": \"*\"}})\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "\n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "\n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "\n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "\n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "\n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "\n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Initialize the Ollama client and the base agent\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "def _build_cors_preflight_response():\n",
    "    response = jsonify({\"status\": \"preflight\"})\n",
    "    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n",
    "    response.headers.add(\"Access-Control-Allow-Headers\", \"Content-Type\")\n",
    "    response.headers.add(\"Access-Control-Allow-Methods\", \"POST, OPTIONS\")\n",
    "    return response\n",
    "\n",
    "def _corsify_actual_response(response):\n",
    "    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n",
    "    return response\n",
    "\n",
    "@app.route('/generate', methods=['POST', 'OPTIONS'])\n",
    "def generate():\n",
    "    if request.method == 'OPTIONS':\n",
    "        return _build_cors_preflight_response()\n",
    "    elif request.method == 'POST':\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "        response = agent.safe_generate(prompt)\n",
    "        return _corsify_actual_response(jsonify(response.__dict__))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 17:52:41,562 - INFO - Starting production server on port 5000\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 48] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 199\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwaitress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serve\n\u001b[1;32m    198\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting production server on port 5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/waitress/__init__.py:13\u001b[0m, in \u001b[0;36mserve\u001b[0;34m(app, **kw)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quiet:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# idempotent if logging has already been set up\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     logging\u001b[38;5;241m.\u001b[39mbasicConfig()\n\u001b[0;32m---> 13\u001b[0m server \u001b[38;5;241m=\u001b[39m \u001b[43m_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quiet:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     server\u001b[38;5;241m.\u001b[39mprint_listen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServing on http://\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/waitress/server.py:78\u001b[0m, in \u001b[0;36mcreate_server\u001b[0;34m(application, map, _start, _sock, _dispatcher, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adj\u001b[38;5;241m.\u001b[39msockets:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sockinfo \u001b[38;5;129;01min\u001b[39;00m adj\u001b[38;5;241m.\u001b[39mlisten:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# When TcpWSGIServer is called, it registers itself in the map. This\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# side-effect is all we need it for, so we don't store a reference to\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# or return it to the user.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m         last_serv \u001b[38;5;241m=\u001b[39m \u001b[43mTcpWSGIServer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[43mapplication\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdispatcher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdispatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43msockinfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockinfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         effective_listen\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     88\u001b[0m             (last_serv\u001b[38;5;241m.\u001b[39meffective_host, last_serv\u001b[38;5;241m.\u001b[39meffective_port)\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sock \u001b[38;5;129;01min\u001b[39;00m adj\u001b[38;5;241m.\u001b[39msockets:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/waitress/server.py:243\u001b[0m, in \u001b[0;36mBaseWSGIServer.__init__\u001b[0;34m(self, application, map, _start, _sock, dispatcher, adj, sockinfo, bind_socket, **kw)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_reuse_addr()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bind_socket:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_server_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetsockname()\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m adj\u001b[38;5;241m.\u001b[39mserver_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/waitress/server.py:364\u001b[0m, in \u001b[0;36mTcpWSGIServer.bind_server_socket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_server_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     (_, _, _, sockaddr) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msockinfo\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockaddr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/waitress/wasyncore.py:374\u001b[0m, in \u001b[0;36mdispatcher.bind\u001b[0;34m(self, addr)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, addr):\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddr \u001b[38;5;241m=\u001b[39m addr\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 48] Address already in use"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging  \n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={\n",
    "    r\"/generate\": {\n",
    "        \"origins\": [\"http://localhost:3000\"],\n",
    "        \"methods\": [\"POST\", \"OPTIONS\"],\n",
    "        \"allow_headers\": [\"Content-Type\"],\n",
    "        \"supports_credentials\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Production-ready Ollama client with enhanced error handling\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", \n",
    "                 base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 3\n",
    "        self.request_timeout = 120\n",
    "        self.session = requests.Session()\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Robust JSON parsing with multiple fallback strategies\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            for wrapper in ['{}', '[]']:\n",
    "                try:\n",
    "                    return json.loads(wrapper[0] + clean_text + wrapper[1])\n",
    "                except:\n",
    "                    continue\n",
    "            return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with exponential backoff\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = self.session.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.ok:\n",
    "                    models = [m['name'] for m in self._parse_json_safe(resp.text).get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with chunked streaming\"\"\"\n",
    "        try:\n",
    "            with self.session.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            ) as resp:\n",
    "                for chunk in resp.iter_content(chunk_size=None):\n",
    "                    if chunk:\n",
    "                        status = self._parse_json_safe(chunk.decode()).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Production-grade generation with circuit breaker pattern\"\"\"\n",
    "        sanitized_prompt = prompt[:4000].strip()\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = self.session.post(\n",
    "                    f\"{self.base_url}/api/generate\",\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt\": sanitized_prompt,\n",
    "                        \"stream\": False,\n",
    "                        \"options\": {\"temperature\": 0.5}\n",
    "                    },\n",
    "                    timeout=self.request_timeout\n",
    "                )\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "                logger.warning(f\"Generation failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Service unavailable after {self.max_retries} attempts\", True\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Production-ready agent with timeout management\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.max_wait = 90\n",
    "\n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with circuit breaker and time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        response = TherapeuticResponse(\n",
    "            text=\"\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            processing_time=0.0\n",
    "        )\n",
    "\n",
    "        if not isinstance(prompt, str) or not prompt.strip():\n",
    "            response.error_details = \"Invalid input: Empty or non-string prompt\"\n",
    "            return response\n",
    "\n",
    "        try:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(self.client.generate, prompt)\n",
    "                text, error = future.result(timeout=self.max_wait)\n",
    "                \n",
    "                response.text = text\n",
    "                response.error = error\n",
    "                response.processing_time = time.time() - start_time\n",
    "                response.error_details = text if error else \"\"\n",
    "                return response\n",
    "        except FutureTimeoutError:\n",
    "            response.error_details = f\"Timeout after {self.max_wait}s\"\n",
    "            response.timeout = True\n",
    "        except Exception as e:\n",
    "            response.error_details = str(e)\n",
    "        \n",
    "        response.processing_time = time.time() - start_time\n",
    "        return response\n",
    "\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "@app.route('/generate', methods=['POST', 'OPTIONS'])\n",
    "def generate():\n",
    "    if request.method == 'OPTIONS':\n",
    "        return _build_cors_preflight_response()\n",
    "    \n",
    "    data = request.get_json(silent=True) or {}\n",
    "    prompt = data.get('prompt', '')\n",
    "    \n",
    "    if not prompt:\n",
    "        return jsonify({\n",
    "            \"error\": True,\n",
    "            \"text\": \"Empty prompt\",\n",
    "            \"error_details\": \"No prompt provided in request\"\n",
    "        }), 400\n",
    "    \n",
    "    response = agent.safe_generate(prompt)\n",
    "    return _corsify_actual_response(jsonify(response.__dict__))\n",
    "\n",
    "def _build_cors_preflight_response():\n",
    "    response = jsonify({\"status\": \"preflight\"})\n",
    "    response.headers.add(\"Access-Control-Allow-Origin\", \"http://localhost:3000\")\n",
    "    response.headers.add(\"Access-Control-Allow-Headers\", \"Content-Type\")\n",
    "    response.headers.add(\"Access-Control-Allow-Methods\", \"POST, OPTIONS\")\n",
    "    return response\n",
    "\n",
    "def _corsify_actual_response(response):\n",
    "    response.headers.add(\"Access-Control-Allow-Origin\", \"http://localhost:3000\")\n",
    "    response.headers.add(\"Access-Control-Expose-Headers\", \"Content-Type\")\n",
    "    return response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from waitress import serve\n",
    "    logger.info(\"Starting production server on port 5000\")\n",
    "    serve(app, host=\"0.0.0.0\", port=5000, threads=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
