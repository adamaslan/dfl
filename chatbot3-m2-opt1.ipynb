{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 1/1 [00:00<00:00,  5.89it/s]\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fba9793ddec4307bcc62ea7bff06598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea2019d97094103b3da8e1965382bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Epoch 1/3:  53%|█████▎    | 8/15 [24:18<25:17, 216.74s/it, loss=nan]2025-01-03 00:02:39,393 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  60%|██████    | 9/15 [24:18<14:55, 149.18s/it, loss=nan]2025-01-03 00:02:39,521 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  67%|██████▋   | 10/15 [24:18<08:35, 103.16s/it, loss=nan]2025-01-03 00:02:39,636 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  73%|███████▎  | 11/15 [24:18<04:46, 71.62s/it, loss=nan] 2025-01-03 00:02:39,735 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  80%|████████  | 12/15 [24:19<02:29, 49.87s/it, loss=nan]2025-01-03 00:02:39,838 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  87%|████████▋ | 13/15 [24:19<01:09, 34.79s/it, loss=nan]2025-01-03 00:02:39,958 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3:  93%|█████████▎| 14/15 [24:19<00:24, 24.32s/it, loss=nan]2025-01-03 00:02:40,055 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 1/3: 100%|██████████| 15/15 [24:19<00:00, 97.30s/it, loss=nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 50, 'min_length': 1, 'num_beams': 4, 'bad_words_ids': [[0, 0]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "Epoch 2/3:   0%|          | 0/15 [00:00<?, ?it/s]2025-01-03 00:02:45,520 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:   7%|▋         | 1/15 [00:00<00:05,  2.57it/s]2025-01-03 00:02:45,630 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  13%|█▎        | 2/15 [00:00<00:02,  4.46it/s]2025-01-03 00:02:45,745 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  20%|██        | 3/15 [00:00<00:02,  5.77it/s]2025-01-03 00:02:45,869 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  27%|██▋       | 4/15 [00:00<00:01,  6.46it/s]2025-01-03 00:02:45,990 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  33%|███▎      | 5/15 [00:00<00:01,  6.96it/s]2025-01-03 00:02:46,104 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  40%|████      | 6/15 [00:00<00:01,  7.55it/s]2025-01-03 00:02:46,243 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  47%|████▋     | 7/15 [00:01<00:01,  7.22it/s]2025-01-03 00:02:46,374 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  53%|█████▎    | 8/15 [00:01<00:00,  7.45it/s]2025-01-03 00:02:46,484 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  60%|██████    | 9/15 [00:01<00:00,  7.86it/s]2025-01-03 00:02:46,609 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  67%|██████▋   | 10/15 [00:01<00:00,  7.89it/s]2025-01-03 00:02:46,722 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  73%|███████▎  | 11/15 [00:01<00:00,  8.37it/s]2025-01-03 00:02:46,836 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  80%|████████  | 12/15 [00:01<00:00,  8.40it/s]2025-01-03 00:02:46,966 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  87%|████████▋ | 13/15 [00:01<00:00,  8.17it/s]2025-01-03 00:02:47,073 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3:  93%|█████████▎| 14/15 [00:01<00:00,  8.50it/s]2025-01-03 00:02:47,199 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 2/3: 100%|██████████| 15/15 [00:02<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:   0%|          | 0/15 [00:00<?, ?it/s]2025-01-03 00:02:52,405 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:   7%|▋         | 1/15 [00:00<00:12,  1.14it/s]2025-01-03 00:02:52,717 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  13%|█▎        | 2/15 [00:01<00:05,  2.24it/s]2025-01-03 00:02:52,844 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  20%|██        | 3/15 [00:01<00:03,  3.34it/s]2025-01-03 00:02:52,982 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  27%|██▋       | 4/15 [00:01<00:02,  4.31it/s]2025-01-03 00:02:53,103 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  33%|███▎      | 5/15 [00:01<00:01,  5.12it/s]2025-01-03 00:02:53,245 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  40%|████      | 6/15 [00:01<00:01,  5.76it/s]2025-01-03 00:02:53,377 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  47%|████▋     | 7/15 [00:01<00:01,  6.24it/s]2025-01-03 00:02:53,503 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  53%|█████▎    | 8/15 [00:01<00:01,  5.99it/s]2025-01-03 00:02:53,681 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  60%|██████    | 9/15 [00:01<00:00,  6.59it/s]2025-01-03 00:02:53,801 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  67%|██████▋   | 10/15 [00:02<00:00,  7.07it/s]2025-01-03 00:02:53,908 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  73%|███████▎  | 11/15 [00:02<00:00,  7.41it/s]2025-01-03 00:02:54,046 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  80%|████████  | 12/15 [00:02<00:00,  7.29it/s]2025-01-03 00:02:54,183 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  87%|████████▋ | 13/15 [00:02<00:00,  7.49it/s]2025-01-03 00:02:54,294 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3:  93%|█████████▎| 14/15 [00:02<00:00,  7.94it/s]2025-01-03 00:02:54,417 - ERROR - Unexpected error during training: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 768\n",
      "Epoch 3/3: 100%|██████████| 15/15 [00:02<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    RagTokenForGeneration, \n",
    "    RagTokenizer, \n",
    "    RagRetriever, \n",
    "    RagConfig, \n",
    "    DPRQuestionEncoder,\n",
    "    DPRQuestionEncoderTokenizer\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from datasets import Dataset as HFDataset\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gc\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Optimized configuration for 8GB RAM\n",
    "CONFIG = {\n",
    "    \"model_name\": \"facebook/rag-sequence-nq\",\n",
    "    \"question_encoder_name\": \"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    \"max_length\": 128,  # Further reduced\n",
    "    \"batch_size\": 1,    # Minimum batch size\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"dataset_path\": \"custom_dataset\",\n",
    "    \"index_path\": \"custom_index.faiss\",\n",
    "    \"chunk_size\": 50    # Smaller chunks\n",
    "}\n",
    "\n",
    "# Memory management helper\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    if hasattr(torch.mps, 'empty_cache'):\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"custom_dataset\", exist_ok=True)\n",
    "\n",
    "# Load data in chunks\n",
    "def process_data_in_chunks(df, chunk_size=CONFIG[\"chunk_size\"]):\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        chunk_dataset = HFDataset.from_pandas(\n",
    "            pd.DataFrame({\n",
    "                'text': chunk['articles'].tolist(),\n",
    "                'title': [f\"Article {i}\" for i in range(len(chunk))],\n",
    "                'id': list(range(len(chunk)))\n",
    "            })\n",
    "        )\n",
    "        processed_chunks.append(chunk_dataset)\n",
    "        clear_memory()\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "# Load and prepare passages in chunks\n",
    "articles_df = pd.read_csv(\"articles.csv\", usecols=[\"articles\"])\n",
    "dataset_chunks = process_data_in_chunks(articles_df)\n",
    "\n",
    "# Initialize question encoder with memory optimization\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\n",
    "    CONFIG[\"question_encoder_name\"],\n",
    "    torch_dtype=torch.float16  # Use half precision\n",
    ")\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(CONFIG[\"question_encoder_name\"])\n",
    "\n",
    "# Optimized embedding computation\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(batch):\n",
    "    try:\n",
    "        encodings = question_encoder_tokenizer(\n",
    "            batch['text'],\n",
    "            max_length=CONFIG[\"max_length\"],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        outputs = question_encoder(\n",
    "            input_ids=encodings['input_ids'],\n",
    "            attention_mask=encodings['attention_mask']\n",
    "        )\n",
    "        embeddings = outputs.pooler_output.cpu().numpy()\n",
    "        clear_memory()\n",
    "        \n",
    "        return {'embeddings': embeddings}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error computing embeddings: {e}\")\n",
    "        return {'embeddings': np.zeros((len(batch['text']), 768))}\n",
    "\n",
    "# Process embeddings in chunks\n",
    "all_embeddings = []\n",
    "for chunk in dataset_chunks:\n",
    "    chunk_with_embeddings = chunk.map(\n",
    "        compute_embeddings,\n",
    "        batched=True,\n",
    "        batch_size=CONFIG[\"batch_size\"]\n",
    "    )\n",
    "    all_embeddings.extend(chunk_with_embeddings['embeddings'])\n",
    "    clear_memory()\n",
    "\n",
    "# Create and save FAISS index\n",
    "dimension = 768\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "embeddings_array = np.array(all_embeddings, dtype=np.float32)\n",
    "index.add(embeddings_array)\n",
    "faiss.write_index(index, CONFIG[\"index_path\"])\n",
    "clear_memory()\n",
    "\n",
    "# Save processed dataset\n",
    "combined_dataset = HFDataset.from_pandas(\n",
    "    pd.DataFrame({\n",
    "        'text': articles_df['articles'].tolist(),\n",
    "        'title': [f\"Article {i}\" for i in range(len(articles_df))],\n",
    "        'id': list(range(len(articles_df))),\n",
    "        'embeddings': all_embeddings\n",
    "    })\n",
    ")\n",
    "combined_dataset.save_to_disk(CONFIG[\"dataset_path\"])\n",
    "clear_memory()\n",
    "\n",
    "# Optimized Dataset class\n",
    "class CQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for chunk_start in range(0, len(df), CONFIG[\"chunk_size\"]):\n",
    "            chunk = df[chunk_start:chunk_start + CONFIG[\"chunk_size\"]]\n",
    "            self._process_chunk(chunk)\n",
    "            clear_memory()\n",
    "    \n",
    "    def _process_chunk(self, chunk):\n",
    "        for _, row in chunk.iterrows():\n",
    "            try:\n",
    "                article = row[\"articles\"]\n",
    "                question = \"What is the content of this article?\"\n",
    "                \n",
    "                inputs = self.tokenizer.question_encoder(\n",
    "                    question,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                targets = self.tokenizer(\n",
    "                    article,\n",
    "                    max_length=self.max_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                self.inputs.append({\n",
    "                    \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "                    \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "                })\n",
    "                self.targets.append(targets[\"input_ids\"].squeeze())\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx][\"input_ids\"],\n",
    "            \"attention_mask\": self.inputs[idx][\"attention_mask\"],\n",
    "            \"labels\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "# Initialize model components with memory optimization\n",
    "tokenizer = RagTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "config = RagConfig.from_pretrained(CONFIG[\"model_name\"])\n",
    "config.index_name = \"custom\"\n",
    "config.passages_path = CONFIG[\"dataset_path\"]\n",
    "config.index_path = CONFIG[\"index_path\"]\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    index_name=\"custom\",\n",
    "    passages_path=CONFIG[\"dataset_path\"],\n",
    "    index_path=CONFIG[\"index_path\"],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize model with memory optimization\n",
    "model = RagTokenForGeneration.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    config=config,\n",
    "    torch_dtype=torch.float16  # Use half precision\n",
    ")\n",
    "model.set_retriever(retriever)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"mps\")  # Use Metal Performance Shaders for M2 Mac\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer with gradient clipping\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Gradient accumulation steps\n",
    "gradient_accumulation_steps = 8  # Increased for better memory handling\n",
    "\n",
    "# Training loop with proper loss handling and error recovery\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_dataset = CQADataset(articles_df, tokenizer, max_length=CONFIG[\"max_length\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass with gradient computation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Ensure loss is a scalar tensor\n",
    "            loss = outputs.loss\n",
    "            if not isinstance(loss, torch.Tensor):\n",
    "                loss = torch.tensor(loss, requires_grad=True, device=device)\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            scaled_loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            scaled_loss.backward()\n",
    "            \n",
    "            if (i + 1) % gradient_accumulation_steps == 0:\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                clear_memory()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            progress_bar.set_postfix({'loss': current_loss})\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            logging.error(f\"Runtime error during training: {e}\")\n",
    "            optimizer.zero_grad()\n",
    "            clear_memory()\n",
    "            continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error during training: {e}\")\n",
    "            optimizer.zero_grad()\n",
    "            clear_memory()\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Clear memory between epochs\n",
    "    clear_memory()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        checkpoint_dir = f\"models/checkpoint-epoch-{epoch + 1}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "# Save the final model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model.save_pretrained(\"models/rag_model_final\")\n",
    "tokenizer.save_pretrained(\"models/rag_model_final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
